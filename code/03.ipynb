{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# SVM: Support Vector Machines\n",
    "\n",
    "For this case the data is imported from sklearn datasets.  \n",
    "It is easier to use so it is good to test the models directly.  \n",
    "However since it is so easy to use and it is already prepared, it is unreal for a real case\n",
    "where data comes in irregular shape and with missing or wrong values.\n",
    "\n",
    "SVM does classification: it predicts if the data belongs to a certain class (maybe like KNN?)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And import metrics and KNN to compare them\n",
    "from sklearn import metrics \n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "source": [
    "## Get the data\n",
    "\n",
    "We're going to use a breast cancer dataset, which is ok for this method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n 'mean smoothness' 'mean compactness' 'mean concavity'\n 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n 'radius error' 'texture error' 'perimeter error' 'area error'\n 'smoothness error' 'compactness error' 'concavity error'\n 'concave points error' 'symmetry error' 'fractal dimension error'\n 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n 'worst smoothness' 'worst compactness' 'worst concavity'\n 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "cancer = datasets.load_breast_cancer()\n",
    "print(type(cancer))\n",
    "print(cancer.feature_names)\n",
    "print(cancer.target_names)"
   ]
  },
  {
   "source": [
    "## Split into train and test batches"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cancer.data\n",
    "y = cancer.target\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.2) "
   ]
  },
  {
   "source": [
    "We took 20% of the data as the training data.  \n",
    "We can play with than number but Tim's recommendation is to not go over 30%."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1.327e+01 1.702e+01 8.455e+01 ... 9.678e-02 2.506e-01 7.623e-02]\n [1.246e+01 1.989e+01 8.043e+01 ... 7.625e-02 2.685e-01 7.764e-02]\n [1.134e+01 1.861e+01 7.276e+01 ... 8.542e-02 3.060e-01 6.783e-02]\n ...\n [1.390e+01 1.924e+01 8.873e+01 ... 8.150e-02 2.356e-01 7.603e-02]\n [2.031e+01 2.706e+01 1.329e+02 ... 1.697e-01 3.151e-01 7.999e-02]\n [1.181e+01 1.739e+01 7.527e+01 ... 4.306e-02 3.200e-01 6.576e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1 1 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 0\n 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0\n 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0\n 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0\n 0 1 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1\n 0 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1\n 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1\n 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 0 1 0\n 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1\n 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0\n 1 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1\n 1 0 0 1 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "source": [
    "1 Represents 'benign', 0 represents 'malign'."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Create and fit the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "source": [
    "## Predict"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "source": [
    "In Tim's video the `acc` value was crap (approx 50%) because the lack of parameters passed to SVC().  \n",
    "I don't know why in my case the result is so good (93%)!  \n",
    "Let's try to repeat the process and see what happens:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9385964912280702\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.2) \n",
    "clf = svm.SVC()\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "source": [
    "Even better!!!  \n",
    "It may very well be the case that sklearn improved its performance with default parameters.\n",
    "\n",
    "Reading some comments in the video, it seems that what I had guessed is correct:\n",
    "\n",
    "> LACIEE mai:  \n",
    "> 1 week ago  \n",
    "> I have the same thing. that's probably because gamma default value was changed from \"auto\" to \"scale\" (I found that in documentation). if you try svm.SVC(gamma=\"auto\") you'll have a low accuracy as Tim has.\n",
    "\n",
    "Let's check it out:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.631578947368421\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.2) \n",
    "clf = svm.SVC(gamma=\"auto\")\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "source": [
    "Ok great, now that we are on the same page as Tim, let's keep following the tutorial:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9385964912280702\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.2) \n",
    "clf = svm.SVC(gamma=\"auto\", kernel=\"linear\")\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "source": [
    "That is a great result comparing with the 61% from the previous attempt."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial is a good kernel (maybe) but Tim says it takes a lot of time to train \n",
    "#   while the previous attempts were a matter of a few seconds\n",
    "if False:\n",
    "    import time \n",
    "    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.2) \n",
    "    clf = svm.SVC(gamma=\"auto\", kernel=\"poly\")\n",
    "    start = time.time()\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"fit time: \", time.time() - start)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(acc)"
   ]
  },
  {
   "source": [
    "That just never ends... I killed it.\n",
    "\n",
    "If I remove `gamma=\"auto\"` it runs very fast, since Tim's tutorial was in 2019, it seems that sklearn has improved a lot since...\n",
    "\n",
    "Now let's see the *soft margin* option (parameter `C`):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.2) \n",
    "clf = svm.SVC(gamma=\"auto\", kernel=\"linear\", C=2)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "source": [
    "The accuracy improves, but it is also slower to train (I didn't measure but felt it)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Compare with KNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn.fit(x_train, y_train)\n",
    "knn_y_pred = knn.predict(x_test)\n",
    "knn_acc = metrics.accuracy_score(y_test, knn_y_pred)\n",
    "print(knn_acc)"
   ]
  },
  {
   "source": [
    "95% is ok!\n",
    "\n",
    "Still SVM is usually better, I should tweak around the parameters to see how much I can improve SVM."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel=\"linear\", C=1)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "source": [
    "And it worked much better!\n",
    "\n",
    "![Yeah science!](../images/yeah_science.jpeg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}